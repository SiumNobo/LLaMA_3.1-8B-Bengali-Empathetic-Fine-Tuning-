{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f9bb19-84f5-4bc9-bf3f-9d7791425347",
   "metadata": {},
   "source": [
    "# LLaMA 3.1-8B Bengali Empathetic Fine-Tuning \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b70cc-f362-4cd0-b1c1-2f275b81273c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:26.416894Z",
     "iopub.status.busy": "2026-01-05T03:20:26.416580Z",
     "iopub.status.idle": "2026-01-05T03:20:43.668918Z",
     "shell.execute_reply": "2026-01-05T03:20:43.668158Z",
     "shell.execute_reply.started": "2026-01-05T03:20:26.416873Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.1rc0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ INSTALL COMPLETE\n",
      "‚ö†Ô∏è  RESTART KERNEL NOW: Kernel ‚Üí Restart & Clear Output\n",
      "   Then run from CELL 2 (skip CELL 1)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================ CELL 1 ============================\n",
    "# Install dependencies (Run once, then restart kernel)\n",
    "\n",
    "!pip install -U transformers>=4.44.0\n",
    "!pip install -U accelerate>=0.27.0\n",
    "!pip install -U peft>=0.7.0\n",
    "!pip install -U bitsandbytes>=0.43.0\n",
    "!pip install sentencepiece tqdm evaluate sacrebleu rouge-score\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSTALL COMPLETE\")\n",
    "print(\" RESTART KERNEL NOW: Kernel ‚Üí Restart & Clear Output\")\n",
    "print(\"   Then run from CELL 2 (skip CELL 1)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecacbd-48f1-4620-9f54-4e0e02102060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:43.672584Z",
     "iopub.status.busy": "2026-01-05T03:20:43.672315Z",
     "iopub.status.idle": "2026-01-05T03:20:59.534430Z",
     "shell.execute_reply": "2026-01-05T03:20:59.533771Z",
     "shell.execute_reply.started": "2026-01-05T03:20:43.672546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformers version: 4.57.3\n",
      "‚úÖ Accelerate version: 1.12.0\n",
      "‚úÖ BitsAndBytes available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 03:20:53.702048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767583253.723387     447 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767583253.730100     447 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767583253.747275     447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767583253.747295     447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767583253.747297     447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767583253.747299     447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ PyTorch: 2.8.0+cu126\n",
      "‚úÖ CUDA available: True\n",
      "‚úÖ GPU: Tesla T4\n",
      "‚úÖ GPU Memory: 14.7 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================ CELL 2 ============================\n",
    "# Imports and environment checks\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import accelerate\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "\n",
    "# Check BitsAndBytes\n",
    "QUANTIZATION_AVAILABLE = False\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    QUANTIZATION_AVAILABLE = True\n",
    "    print(\"BitsAndBytes available\")\n",
    "except Exception as e:\n",
    "    print(f\"BitsAndBytes NOT available: {e}\")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "import evaluate\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\" GPU Memory: {total_mem:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f7777b-1aef-4d1c-adb9-ba3b31a27c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:59.537028Z",
     "iopub.status.busy": "2026-01-05T03:20:59.536428Z",
     "iopub.status.idle": "2026-01-05T03:20:59.753825Z",
     "shell.execute_reply": "2026-01-05T03:20:59.753087Z",
     "shell.execute_reply.started": "2026-01-05T03:20:59.537001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HF token loaded from Kaggle Secrets\n",
      "‚úÖ Logged in to HuggingFace\n"
     ]
    }
   ],
   "source": [
    "# ============================ CELL 3 ============================\n",
    "# HuggingFace Authentication\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = None\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    print(\"‚úÖ HF token loaded from Kaggle Secrets\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Kaggle Secrets error: {e}\")\n",
    "\n",
    "if not hf_token:\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        print(\"‚úÖ HF token from env\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token, new_session=False)\n",
    "    print(\"‚úÖ Logged in to HuggingFace\")\n",
    "else:\n",
    "    raise ValueError(\"HF_TOKEN not found. Add it to Kaggle Secrets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0418289-59f8-4d5f-98e3-eaaad090082f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:59.755107Z",
     "iopub.status.busy": "2026-01-05T03:20:59.754809Z",
     "iopub.status.idle": "2026-01-05T03:20:59.764090Z",
     "shell.execute_reply": "2026-01-05T03:20:59.763505Z",
     "shell.execute_reply.started": "2026-01-05T03:20:59.755074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° FAST CONFIG READY\n",
      "   Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "   Max samples: 3000\n",
      "   Max length: 256\n",
      "   Epochs: 1\n",
      "   LoRA rank: 8\n",
      "   Target modules: ['q_proj', 'v_proj']\n",
      "\n",
      "‚è±Ô∏è Estimated time: ~2-3 hours\n"
     ]
    }
   ],
   "source": [
    "# ============================ CELL 4 ============================\n",
    "# ‚ö° FAST Configuration - Will finish in ~2-3 hours\n",
    "\n",
    "class Config:\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    \n",
    "    # ‚ö° SPEED OPTIMIZED SETTINGS\n",
    "    MAX_LENGTH = 256              # ‚ö° Reduced from 2048\n",
    "    NUM_EPOCHS = 1                # ‚ö° Reduced from 3\n",
    "    MAX_SAMPLES = 3000            # ‚ö° Limit data samples\n",
    "    \n",
    "    # LoRA settings (smaller for speed)\n",
    "    LORA_R = 8                    # ‚ö° Reduced from 16\n",
    "    LORA_ALPHA = 16               # ‚ö° Reduced from 32\n",
    "    LORA_DROPOUT = 0.05\n",
    "    TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # ‚ö° Reduced from 4 modules\n",
    "\n",
    "    BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4  # ‚ö° Reduced from 8\n",
    "    LEARNING_RATE = 3e-4          # ‚ö° Slightly higher for faster learning\n",
    "    WARMUP_STEPS = 10\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "\n",
    "    TRAIN_SPLIT = 0.9             # 90% train, 10% val\n",
    "    SEED = 42\n",
    "\n",
    "    DATASET_PATH_PRIMARY = \"/kaggle/input/bengalitext/BengaliEmpatheticConversationsCorpus.csv\"\n",
    "    DATASET_PATH_FALLBACK = \"/kaggle/input/bengalitext/BengaliEmpatheticConversationsCorpus .csv\"\n",
    "\n",
    "    OUTPUT_DIR = \"./outputs\"\n",
    "    DB_PATH = \"./llama_logs.db\"\n",
    "    HUMAN_EVAL_CSV = \"./human_eval_sheet.csv\"\n",
    "\n",
    "config = Config()\n",
    "torch.manual_seed(config.SEED)\n",
    "np.random.seed(config.SEED)\n",
    "\n",
    "print(\"‚ö° FAST CONFIG READY\")\n",
    "print(f\"   Model: {config.MODEL_NAME}\")\n",
    "print(f\"   Max samples: {config.MAX_SAMPLES}\")\n",
    "print(f\"   Max length: {config.MAX_LENGTH}\")\n",
    "print(f\"   Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"   LoRA rank: {config.LORA_R}\")\n",
    "print(f\"   Target modules: {config.TARGET_MODULES}\")\n",
    "print(\"\\n‚è±Ô∏è Estimated time: ~2-3 hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e55b489-84c8-4d92-a952-9698cd85ccef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:59.765178Z",
     "iopub.status.busy": "2026-01-05T03:20:59.764840Z",
     "iopub.status.idle": "2026-01-05T03:20:59.779475Z",
     "shell.execute_reply": "2026-01-05T03:20:59.778675Z",
     "shell.execute_reply.started": "2026-01-05T03:20:59.765145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================ CELL 5 ============================\n",
    "# Data structures + Dataset Processor\n",
    "\n",
    "@dataclass\n",
    "class ConversationPair:\n",
    "    topic: str\n",
    "    question_title: str\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class DatasetProcessor:\n",
    "    REQUIRED_COLS = [\"Topics\", \"Question-Title\", \"Questions\", \"Answers\"]\n",
    "\n",
    "    def __init__(self, dataset_path: Optional[str] = None):\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def resolve_path(self) -> str:\n",
    "        if self.dataset_path and os.path.exists(self.dataset_path):\n",
    "            return self.dataset_path\n",
    "        if os.path.exists(config.DATASET_PATH_PRIMARY):\n",
    "            return config.DATASET_PATH_PRIMARY\n",
    "        if os.path.exists(config.DATASET_PATH_FALLBACK):\n",
    "            return config.DATASET_PATH_FALLBACK\n",
    "        \n",
    "        input_dir = \"/kaggle/input/bengalitext\"\n",
    "        if os.path.exists(input_dir):\n",
    "            print(f\"Available files: {os.listdir(input_dir)}\")\n",
    "        raise FileNotFoundError(\"Dataset not found\")\n",
    "\n",
    "    def load(self) -> List[ConversationPair]:\n",
    "        path = self.resolve_path()\n",
    "        print(f\"üì• Loading dataset from: {path}\")\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "\n",
    "        missing = [c for c in self.REQUIRED_COLS if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "        conversations = [\n",
    "            ConversationPair(\n",
    "                topic=str(row[\"Topics\"]),\n",
    "                question_title=str(row[\"Question-Title\"]),\n",
    "                question=str(row[\"Questions\"]),\n",
    "                answer=str(row[\"Answers\"]),\n",
    "            )\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # ‚ö° LIMIT SAMPLES FOR SPEED\n",
    "        conversations = conversations[:config.MAX_SAMPLES]\n",
    "        print(f\"‚ö° Limited to {len(conversations)} samples for speed\")\n",
    "        return conversations\n",
    "\n",
    "    def split(self, conversations: List[ConversationPair]) -> Tuple[List[ConversationPair], List[ConversationPair]]:\n",
    "        split_idx = int(len(conversations) * config.TRAIN_SPLIT)\n",
    "        train_convs = conversations[:split_idx]\n",
    "        val_convs = conversations[split_idx:]\n",
    "        print(f\"‚úÖ Train: {len(train_convs)} | Val: {len(val_convs)}\")\n",
    "        return train_convs, val_convs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2251770-98c2-4379-b747-c2c84950ef9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:59.780673Z",
     "iopub.status.busy": "2026-01-05T03:20:59.780414Z",
     "iopub.status.idle": "2026-01-05T03:20:59.794536Z",
     "shell.execute_reply": "2026-01-05T03:20:59.793785Z",
     "shell.execute_reply.started": "2026-01-05T03:20:59.780632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================ CELL 6 ============================\n",
    "# Dataset class\n",
    "\n",
    "class BengaliEmpatheticDataset(Dataset):\n",
    "    def __init__(self, conversations: List[ConversationPair], tokenizer: AutoTokenizer, max_length: int):\n",
    "        self.conversations = conversations\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prompt(topic: str, question: str) -> str:\n",
    "        system_prompt = \"‡¶Ü‡¶™‡¶®‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶∏‡¶π‡¶æ‡¶®‡ßÅ‡¶≠‡ßÇ‡¶§‡¶ø‡¶∂‡ßÄ‡¶≤ ‡¶™‡¶∞‡¶æ‡¶Æ‡¶∞‡ßç‡¶∂‡¶¶‡¶æ‡¶§‡¶æ ‡¶Ø‡¶ø‡¶®‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡ßá‡¶®‡•§\"\n",
    "        return (\n",
    "            \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            f\"{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º: {topic}\\n\"\n",
    "            f\"‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        conv = self.conversations[idx]\n",
    "        prompt = self.build_prompt(conv.topic, conv.question)\n",
    "        answer = f\"{conv.answer}<|eot_id|>\"\n",
    "\n",
    "        prompt_ids = self.tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "        answer_ids = self.tokenizer(answer, add_special_tokens=False).input_ids\n",
    "\n",
    "        input_ids = (prompt_ids + answer_ids)[: self.max_length]\n",
    "        labels = ([-100] * len(prompt_ids) + answer_ids)[: self.max_length]\n",
    "        attn_mask = [1] * len(input_ids)\n",
    "\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        pad_len = self.max_length - len(input_ids)\n",
    "        if pad_len > 0:\n",
    "            input_ids += [pad_id] * pad_len\n",
    "            labels += [-100] * pad_len\n",
    "            attn_mask += [0] * pad_len\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72dad60-10e7-4b41-b777-32386bcf8f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:59.795624Z",
     "iopub.status.busy": "2026-01-05T03:20:59.795385Z",
     "iopub.status.idle": "2026-01-05T03:20:59.810327Z",
     "shell.execute_reply": "2026-01-05T03:20:59.809751Z",
     "shell.execute_reply.started": "2026-01-05T03:20:59.795591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================ CELL 7 ============================\n",
    "# Model Loading with proper gradient setup\n",
    "\n",
    "def load_model_and_apply_lora():\n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING MODEL\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    model = None\n",
    "    \n",
    "    if QUANTIZATION_AVAILABLE:\n",
    "        try:\n",
    "            print(\"Attempting 4-bit quantization...\")\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                config.MODEL_NAME,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            print(\" Loaded with 4-bit quantization\")\n",
    "            \n",
    "            print(\"Preparing model for k-bit training...\")\n",
    "            model = prepare_model_for_kbit_training(\n",
    "                model,\n",
    "                use_gradient_checkpointing=True,\n",
    "                gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    "            )\n",
    "            print(\"Model prepared for training\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"4-bit load failed: {e}\")\n",
    "            model = None\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Loading with FP16...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "        print(\"Loaded with FP16\")\n",
    "\n",
    "    # Apply LoRA\n",
    "    print(\"\\nApplying LoRA adapter...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.LORA_R,\n",
    "        lora_alpha=config.LORA_ALPHA,\n",
    "        target_modules=config.TARGET_MODULES,\n",
    "        lora_dropout=config.LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LoRA APPLIED\")\n",
    "    print(f\"   Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "    print(f\"   Total: {total:,}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b48d47-6b9e-47e9-976d-f4eb357038a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:59.811411Z",
     "iopub.status.busy": "2026-01-05T03:20:59.811168Z",
     "iopub.status.idle": "2026-01-05T03:20:59.826132Z",
     "shell.execute_reply": "2026-01-05T03:20:59.825601Z",
     "shell.execute_reply.started": "2026-01-05T03:20:59.811362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================ CELL 8 ============================\n",
    "# Evaluator\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, tokenizer: AutoTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_perplexity(val_loss: float) -> float:\n",
    "        return float(np.exp(val_loss))\n",
    "\n",
    "    def generate_one(self, model, topic: str, question: str, max_new_tokens: int = 128) -> str:\n",
    "        prompt = BengaliEmpatheticDataset.build_prompt(topic, question)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        decoded = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        if prompt in decoded:\n",
    "            decoded = decoded.split(prompt, 1)[-1].strip()\n",
    "        return decoded.strip()\n",
    "\n",
    "    def compute_bleu_rouge(self, model, val_convs: List[ConversationPair], n: int = 20) -> Dict[str, float]:\n",
    "        sample = val_convs[:min(n, len(val_convs))]\n",
    "        preds, refs = [], []\n",
    "\n",
    "        for c in tqdm(sample, desc=\"Generating for BLEU/ROUGE\"):\n",
    "            pred = self.generate_one(model, c.topic, c.question, max_new_tokens=128)\n",
    "            preds.append(pred)\n",
    "            refs.append(c.answer)\n",
    "\n",
    "        bleu = bleu_metric.compute(predictions=preds, references=[[r] for r in refs])[\"score\"]\n",
    "        rouge = rouge_metric.compute(predictions=preds, references=refs)\n",
    "\n",
    "        return {\n",
    "            \"BLEU\": float(bleu),\n",
    "            \"ROUGE-1\": float(rouge[\"rouge1\"]),\n",
    "            \"ROUGE-2\": float(rouge[\"rouge2\"]),\n",
    "            \"ROUGE-L\": float(rouge[\"rougeL\"]),\n",
    "        }\n",
    "\n",
    "    def export_human_eval_sheet(self, model, val_convs: List[ConversationPair], out_csv: str, n: int = 15) -> str:\n",
    "        sample = val_convs[:min(n, len(val_convs))]\n",
    "        rows = []\n",
    "        for c in tqdm(sample, desc=\"Human eval sheet\"):\n",
    "            pred = self.generate_one(model, c.topic, c.question, max_new_tokens=128)\n",
    "            rows.append({\n",
    "                \"topic\": c.topic,\n",
    "                \"question\": c.question,\n",
    "                \"reference_answer\": c.answer,\n",
    "                \"generated_answer\": pred,\n",
    "                \"empathy_score_1to5\": \"\",\n",
    "                \"helpfulness_score_1to5\": \"\",\n",
    "                \"safety_score_1to5\": \"\",\n",
    "                \"notes\": \"\",\n",
    "            })\n",
    "        pd.DataFrame(rows).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "        return out_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982ab43b-00be-4a5b-8f61-c466dc6879ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:59.827021Z",
     "iopub.status.busy": "2026-01-05T03:20:59.826843Z",
     "iopub.status.idle": "2026-01-05T03:20:59.840438Z",
     "shell.execute_reply": "2026-01-05T03:20:59.839894Z",
     "shell.execute_reply.started": "2026-01-05T03:20:59.827004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================ CELL 9 ============================\n",
    "# Experiment Logger\n",
    "\n",
    "class ExperimentLogger:\n",
    "    def __init__(self, db_path: str):\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self._init_tables()\n",
    "\n",
    "    def _init_tables(self):\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute('''CREATE TABLE IF NOT EXISTS LLAMAExperiments (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            model_name TEXT, lora_config TEXT, train_loss REAL,\n",
    "            val_loss REAL, metrics TEXT, timestamp TEXT\n",
    "        )''')\n",
    "        cur.execute('''CREATE TABLE IF NOT EXISTS GeneratedResponses (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            experiment_id INTEGER, input_text TEXT, response_text TEXT, timestamp TEXT,\n",
    "            FOREIGN KEY(experiment_id) REFERENCES LLAMAExperiments(id)\n",
    "        )''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    def log_experiment(self, model_name: str, lora_config: Dict, train_loss: float, val_loss: float, metrics: Dict) -> int:\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute('''INSERT INTO LLAMAExperiments (model_name, lora_config, train_loss, val_loss, metrics, timestamp)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)''',\n",
    "            (model_name, json.dumps(lora_config), float(train_loss), float(val_loss),\n",
    "             json.dumps(metrics), datetime.utcnow().isoformat()))\n",
    "        self.conn.commit()\n",
    "        return int(cur.lastrowid)\n",
    "\n",
    "    def log_response(self, experiment_id: int, input_text: str, response_text: str):\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute('''INSERT INTO GeneratedResponses (experiment_id, input_text, response_text, timestamp)\n",
    "            VALUES (?, ?, ?, ?)''', (experiment_id, input_text, response_text, datetime.utcnow().isoformat()))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def close(self):\n",
    "        self.conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e0610-e7cb-4740-b10d-bf93b18f0fa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T03:20:59.841680Z",
     "iopub.status.busy": "2026-01-05T03:20:59.841421Z",
     "iopub.status.idle": "2026-01-05T03:20:59.859846Z",
     "shell.execute_reply": "2026-01-05T03:20:59.859163Z",
     "shell.execute_reply.started": "2026-01-05T03:20:59.841659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================ CELL 10 ============================\n",
    "# FineTuner\n",
    "\n",
    "class LLAMAFineTuner:\n",
    "    def __init__(self, model, tokenizer, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "\n",
    "    def setup(self):\n",
    "        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        print(f\"Number of trainable parameter groups: {len(trainable_params)}\")\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            trainable_params,\n",
    "            lr=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        total_steps = (len(self.train_loader) * config.NUM_EPOCHS) // config.GRADIENT_ACCUMULATION_STEPS\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=config.WARMUP_STEPS,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "        print(f\"‚úÖ Optimizer ready. Total steps: {total_steps}\")\n",
    "\n",
    "    def train_one_epoch(self, epoch: int) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        bar = tqdm(self.train_loader, desc=f\"Train Epoch {epoch}\")\n",
    "        for step, batch in enumerate(bar):\n",
    "            input_ids = batch[\"input_ids\"].to(self.model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.model.device)\n",
    "            labels = batch[\"labels\"].to(self.model.device)\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                labels=labels,\n",
    "                use_cache=False\n",
    "            )\n",
    "            loss = outputs.loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "            total_loss += loss.item() * config.GRADIENT_ACCUMULATION_STEPS\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), config.MAX_GRAD_NORM)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            bar.set_postfix({\"loss\": f\"{loss.item() * config.GRADIENT_ACCUMULATION_STEPS:.4f}\"})\n",
    "\n",
    "        return total_loss / max(1, len(self.train_loader))\n",
    "\n",
    "    def validate(self) -> float:\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validate\"):\n",
    "                input_ids = batch[\"input_ids\"].to(self.model.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.model.device)\n",
    "                labels = batch[\"labels\"].to(self.model.device)\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                total_loss += outputs.loss.item()\n",
    "        return total_loss / max(1, len(self.val_loader))\n",
    "\n",
    "    def train(self) -> Dict[str, List[float]]:\n",
    "        history = {\"train_loss\": [], \"val_loss\": []}\n",
    "        for epoch in range(1, config.NUM_EPOCHS + 1):\n",
    "            print(f\"\\n{'='*80}\\nEPOCH {epoch}/{config.NUM_EPOCHS}\\n{'='*80}\")\n",
    "            tr = self.train_one_epoch(epoch)\n",
    "            vl = self.validate()\n",
    "            history[\"train_loss\"].append(float(tr))\n",
    "            history[\"val_loss\"].append(float(vl))\n",
    "            print(f\"\\nEpoch {epoch}: Train={tr:.4f}, Val={vl:.4f}\")\n",
    "        return history\n",
    "\n",
    "    def save(self, out_dir: str):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(out_dir)\n",
    "        self.tokenizer.save_pretrained(out_dir)\n",
    "        print(f\" Saved to: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e9aec-c065-4cab-ae82-99b2fecb2a42",
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-05T10:24:51.498Z",
     "iopub.execute_input": "2026-01-05T03:20:59.860801Z",
     "iopub.status.busy": "2026-01-05T03:20:59.860566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading dataset from: /kaggle/input/bengalitext/BengaliEmpatheticConversationsCorpus .csv\n",
      "‚ö° Limited to 3000 samples for speed\n",
      "‚úÖ Train: 2700 | Val: 300\n",
      "\n",
      "üî§ Loading tokenizer...\n",
      "‚úÖ Tokenizer loaded (vocab: 128,256)\n",
      "================================================================================\n",
      "LOADING MODEL\n",
      "================================================================================\n",
      "Attempting 4-bit quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1ca295fcdd429d8ffe0498d0757ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded with 4-bit quantization\n",
      "Preparing model for k-bit training...\n",
      "‚úÖ Model prepared for training\n",
      "\n",
      "Applying LoRA adapter...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ LoRA APPLIED\n",
      "   Trainable: 3,407,872 (0.07%)\n",
      "   Total: 4,544,008,192\n",
      "================================================================================\n",
      "\n",
      "Number of trainable parameter groups: 128\n",
      "‚úÖ Optimizer ready. Total steps: 675\n",
      "\n",
      "================================================================================\n",
      "EPOCH 1/1\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1:  30%|‚ñà‚ñà‚ñà       | 814/2700 [16:36<38:15,  1.22s/it, loss=nan]   "
     ]
    }
   ],
   "source": [
    "# ============================ CELL 11 ============================\n",
    "# Main pipeline\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    processor = DatasetProcessor()\n",
    "    conversations = processor.load()\n",
    "    train_convs, val_convs = processor.split(conversations)\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"\\nLoading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(f\" Tokenizer loaded (vocab: {len(tokenizer):,})\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_ds = BengaliEmpatheticDataset(train_convs, tokenizer, config.MAX_LENGTH)\n",
    "    val_ds = BengaliEmpatheticDataset(val_convs, tokenizer, config.MAX_LENGTH)\n",
    "    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Load model with LoRA\n",
    "    model = load_model_and_apply_lora()\n",
    "\n",
    "    # Setup and train\n",
    "    finetuner = LLAMAFineTuner(model, tokenizer, train_loader, val_loader)\n",
    "    finetuner.setup()\n",
    "    history = finetuner.train()\n",
    "\n",
    "    # Evaluate\n",
    "    evaluator = Evaluator(tokenizer)\n",
    "    final_train_loss = history[\"train_loss\"][-1]\n",
    "    final_val_loss = history[\"val_loss\"][-1]\n",
    "    perplexity = evaluator.compute_perplexity(final_val_loss)\n",
    "\n",
    "    print(f\"\\n{'='*80}\\nEVALUATION\\n{'='*80}\")\n",
    "    print(f\"Val loss: {final_val_loss:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # ‚ö° FAST: Only 20 samples for BLEU/ROUGE\n",
    "    metrics_text = evaluator.compute_bleu_rouge(model, val_convs, n=20)\n",
    "    metrics = {\"perplexity\": perplexity, **metrics_text}\n",
    "    print(\"\\n Text Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"   {k}: {v:.4f}\")\n",
    "\n",
    "    # ‚ö° FAST: Only 15 samples for human eval\n",
    "    human_csv = evaluator.export_human_eval_sheet(model, val_convs, config.HUMAN_EVAL_CSV, n=15)\n",
    "    print(f\"\\nHuman eval sheet: {human_csv}\")\n",
    "\n",
    "    # Log experiment\n",
    "    logger = ExperimentLogger(config.DB_PATH)\n",
    "    lora_payload = {\n",
    "        \"r\": config.LORA_R, \"alpha\": config.LORA_ALPHA,\n",
    "        \"dropout\": config.LORA_DROPOUT, \"target_modules\": config.TARGET_MODULES,\n",
    "        \"max_length\": config.MAX_LENGTH, \"batch_size\": config.BATCH_SIZE,\n",
    "        \"grad_accum\": config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"lr\": config.LEARNING_RATE, \"epochs\": config.NUM_EPOCHS,\n",
    "    }\n",
    "    experiment_id = logger.log_experiment(config.MODEL_NAME, lora_payload, final_train_loss, final_val_loss, metrics)\n",
    "    print(f\"\\n Logged experiment_id: {experiment_id}\")\n",
    "\n",
    "    # Sample generations\n",
    "    tests = [\n",
    "        {\"topic\": \"‡¶™‡¶æ‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞‡¶ø‡¶ï ‡¶¶‡ßç‡¶¨‡¶®‡ßç‡¶¶‡ßç‡¶¨\", \"question\": \"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶è‡¶¨‡¶Ç ‡¶Æ‡¶æ‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ù‡¶ó‡¶°‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡•§\"},\n",
    "        {\"topic\": \"‡¶â‡¶¶‡ßç‡¶¨‡ßá‡¶ó\", \"question\": \"‡¶Ü‡¶Æ‡¶ø ‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ö‡¶ø‡¶®‡ßç‡¶§‡¶ø‡¶§ ‡¶•‡¶æ‡¶ï‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶ò‡ßÅ‡¶Æ‡¶æ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø ‡¶®‡¶æ‡•§\"},\n",
    "        {\"topic\": \"‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï\", \"question\": \"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ‡¶∞‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶®‡¶æ‡•§\"},\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n{'='*80}\\nSAMPLE GENERATIONS\\n{'='*80}\")\n",
    "    for i, t in enumerate(tests, 1):\n",
    "        prompt = BengaliEmpatheticDataset.build_prompt(t[\"topic\"], t[\"question\"])\n",
    "        response = evaluator.generate_one(model, t[\"topic\"], t[\"question\"], max_new_tokens=128)\n",
    "        print(f\"\\n--- TEST {i} ---\")\n",
    "        print(f\"Topic: {t['topic']}\")\n",
    "        print(f\"Question: {t['question']}\")\n",
    "        print(f\"Response:\\n{response}\")\n",
    "        logger.log_response(experiment_id, prompt, response)\n",
    "\n",
    "    logger.close()\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "    finetuner.save(os.path.join(config.OUTPUT_DIR, \"final_model\"))\n",
    "    with open(os.path.join(config.OUTPUT_DIR, \"history.json\"), \"w\") as f:\n",
    "        json.dump({\"history\": history, \"metrics\": metrics, \"experiment_id\": experiment_id}, f, indent=2)\n",
    "\n",
    "    # Zip for download\n",
    "    !zip -r /kaggle/working/llama_bengali_submission.zip {config.OUTPUT_DIR} {config.DB_PATH} {config.HUMAN_EVAL_CSV}\n",
    "\n",
    "    print(f\"\\n{'='*80}\\nDONE\\n{'='*80}\")\n",
    "    print(\"Download: Output ‚Üí llama_bengali_submission.zip\")\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9184964,
     "sourceId": 14382217,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
